\documentclass{beamer}

\usetheme{Madrid}

\include{lecture-preamble.tex}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

\usepackage[numbers]{natbib}
\setcitestyle{square, numbers}
\bibliographystyle{plainnat}
\setbeamertemplate{bibliography item}[triangle]

\title[Classifying Solutions to a Yang-Baxter-like Equation via Periodic Matrices]{Classifying Solutions to a Yang-Baxter-like Equation via Periodic Matrices}
\author{Mihailo Djurić}
\institute{Mathematical Institute SANU}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Table of Contents}
  \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{The Core Problem}
  We investigate the Yang-Baxter-like Matrix Equation (YBME):
  \begin{block}{The YBME}
  \[ AXA = XAX \]
  \end{block}
  \begin{itemize}
    \item This is a non-linear matrix equation with $n^2$ quadratic equations and $n^2$ variables.
    \item Finding all solutions in general is an open and difficult problem.
  \end{itemize}
\end{frame}

\begin{frame}{Motivation: Why Study This Equation?}
  While the equation $AXA=XAX$ is simple to write down, it is a gateway to deep mathematical concepts.
  \vfill
  \begin{itemize}
    \item \textbf{A Gateway to Quantum Physics:} It is a "classical" or "set-theoretic" version of the celebrated \textbf{Quantum Yang-Baxter Equation} (QYBE).
    \[ R_{12} R_{13} R_{23} = R_{23} R_{13} R_{12} \]
    \item \textbf{Foundations of Modern Physics:} The QYBE is a cornerstone of quantum integrable systems, statistical mechanics, and quantum field theory.
    \vfill
    \item \textbf{Connections to Topology:} Solutions to the Yang-Baxter equation can be used to construct invariants of knots and links, forming a bridge between algebra and low-dimensional topology.
  \end{itemize}
  \vfill
  \begin{alertblock}{Our Goal}
  By studying the simpler matrix equation, we develop tools and intuition for these more complex and significant problems.
  \end{alertblock}
\end{frame}

\begin{frame}{Our Strategy: Add Structure}
  A powerful method in mathematics is to simplify a problem by imposing additional constraints.
  \vfill % Adds vertical space
  \begin{alertblock}{Our Guiding Constraint}
  We will classify solutions where the matrix $A$ has a "generalized periodicity":
  \[ A^k = tA \]
    for an integer $k \ge 2$ and a scalar $t \in \mathbb{C}$.
  \end{alertblock}
\end{frame}

\section{Theoretical Foundations}

\begin{frame}{Annihilating Polynomials}
  Our entire strategy begins with finding a polynomial that a matrix satisfies.
  \begin{definition}
    An \textbf{annihilating polynomial} for a matrix $A$ is any non-zero polynomial $p(x)$ such that $p(A) = 0$.
  \end{definition}
  \vfill
  A fundamental result guarantees that such a polynomial always exists.
  \begin{theorem}[Cayley-Hamilton Theorem]
    Every square matrix satisfies its own characteristic equation. That is, if $p_A(x) = \det(xI - A)$ is the characteristic polynomial of $A$, then $p_A(A) = 0$.
  \end{theorem}
  \vfill
  So, the characteristic polynomial is always an annihilating polynomial.
\end{frame}

\begin{frame}{The Minimal Polynomial}
  The characteristic polynomial is useful, but it is not always the most efficient tool. We want the annihilating polynomial of the \textit{lowest possible degree}.
  \begin{definition}
    The \textbf{minimal polynomial} $m_A(x)$ of a matrix $A$ is the unique monic polynomial of lowest degree that annihilates $A$.
  \end{definition}
  \vfill
  It is the true "genetic code" of the matrix, and it has a crucial relationship with all other annihilating polynomials.
  \begin{theorem}
    The minimal polynomial $m_A(x)$ divides every annihilating polynomial of $A$ (including the characteristic polynomial).
  \end{theorem}
\end{frame}

\begin{frame}{Diagonalizability}
  The key structural property we want to establish is diagonalizability, as it simplifies the YBME immensely.
  \begin{definition}
    A matrix $A$ is \textbf{diagonalizable} if it is similar to a diagonal matrix, i.e., there exists an invertible matrix $P$ such that $A = PDP^{-1}$.
  \end{definition}
  \vfill
  The minimal polynomial provides a direct and powerful criterion for determining this.
  \begin{theorem}[The Main Criterion]
    A matrix is diagonalizable if and only if its minimal polynomial has no repeated roots (i.e., its roots are distinct).
  \end{theorem}
\end{frame}

\begin{frame}{Our Strategy Synthesized}
  These theorems provide a clear, step-by-step strategy to analyze the structure of $A$ from the condition $A^k=tA$.
  \begin{alertblock}{The Strategic Framework}
    \begin{enumerate}
        \item \textbf{Find Annihilator:} The condition $A^k=tA$ means $A$ is annihilated by the polynomial $p(x) = x^k - tx$.
        \pause
        \item \textbf{Check Roots:} Factor $p(x)$ and check if its roots are distinct.
        \pause
        \item \textbf{Constrain Minimal Poly:} Since $m_A(x)$ must divide $p(x)$, if $p(x)$ has distinct roots, then $m_A(x)$ must also have distinct roots.
        \pause
        \item \textbf{Conclude Diagonalizability:} By the main criterion, if $m_A(x)$ has distinct roots, then the matrix $A$ must be diagonalizable.
    \end{enumerate}
  \end{alertblock}
  This logical chain is the engine that drives the first half of our analysis.
\end{frame}

\section{The Foundational Case: $A^3 = -A$}

\begin{frame}{The Foundational Case: $A^3 = -A$}
  This corresponds to our constraint with $k=3$ and $t=-1$.
  \vfill
  The matrix $A$ must be a root of the polynomial:
  \[ p(x) = x^3 + x = 0 \]
  \vfill
  \textbf{The Key Insight:} The properties of $A$ are encoded in this polynomial.
\end{frame}

\begin{frame}{The Key Tool: The Minimal Polynomial}
  \begin{itemize}
    \item The minimal polynomial of $A$, denoted $m_A(x)$, must divide any annihilating polynomial.
    \item Therefore, $m_A(x)$ must divide $p(x) = x^3 + x$.
    \pause % This is an overlay! The next part will appear on the next click.
    \item Let's factor our polynomial:
    \[ p(x) = x(x^2 + 1) = x(x-i)(x+i) \]
    \pause
    \item The roots are $\{0, i, -i\}$. They are distinct.
  \end{itemize}
\end{frame}

\begin{frame}{Diagonalizability}
  We now use a fundamental theorem of linear algebra.
  \begin{theorem}
    A matrix is diagonalizable if and only if its minimal polynomial has no repeated roots.
  \end{theorem}
  \vfill
  \begin{itemize}
    \item Since the roots of $p(x)$ are distinct, the roots of $m_A(x)$ must also be distinct.
    \pause
    \item \textbf{Conclusion:} Any matrix $A$ satisfying $A^3 = -A$ \textbf{must be diagonalizable}.
    \item Its spectrum (set of eigenvalues) must be a subset of $\{0, i, -i\}$.
  \end{itemize}
\end{frame}

\begin{frame}{Applying this to the YBME}
  Now we can solve our original problem for $A^3 = -A$.
  \begin{itemize}
    \item We proved $A$ is diagonalizable with spectrum $\subseteq \{0, i, -i\}$.
    \item A known theorem provides all YBME solutions for matrices with a spectrum $\{0, \lambda, \mu\}$, provided a condition holds. \cite{tripotent}
    \pause
    \item \textbf{The Condition:} $\lambda^2 - \lambda\mu + \mu^2 \neq 0$.
    \pause
    \item \textbf{Our Check:}
    \[ (i)^2 - (i)(-i) + (-i)^2 = -1 - 1 - 1 = -3 \neq 0 \]
    \item The condition holds. Therefore, we can substitute $\lambda=i, \mu=-i$ into the known general solution to get our answer.
  \end{itemize}
\end{frame}

\begin{frame}{Explicit Solution for $A^3 = -A$}
  By substituting $\lambda=i, \mu=-i$ into the general form, we get all solutions $X$:
  \begin{block}{The General Form of $X$}
  % We use \footnotesize to ensure this complex matrix fits on the slide.
  \footnotesize
  \[X = S \begin{bmatrix} P & 0 \\ 0 & Q \end{bmatrix} \left[ \begin{array}{ccc|ccc|c} - \frac{i}{2} I_r & 0 & 0 & F & 0 & 0 & 0 \\ 0 & i I_v & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0_{k-r-v} & 0 & 0 & 0 & C_3 \\ \hline - \frac{3}{4} F^{-1} & 0 & 0 & \frac{i}{2} I_r & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & -i I_t & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0_{m-k-r-t} & C_6 \\ \hline 0 & 0 & D_3 & 0 & 0 & D_6 & W \\ \end{array} \right] \begin{bmatrix} P^{-1} & 0 \\ 0 & Q^{-1} \end{bmatrix} S^{-1}\]
  where $i D_3 C_3 = i D_6 C_6$, and other parameters ($S, P, Q, F, \dots$) are defined as in the theorem.
  \end{block}
  \normalsize % Return to normal font size
  \vfill
  \textbf{Key takeaway:} The structure is complex but completely characterized. The off-diagonal blocks $F$ and its inverse are responsible for the non-commuting solutions.
\end{frame}

\begin{frame}{A Concrete 3x3 Example: Setup}
  Let's make the abstract solution concrete. We will build the simplest non-trivial, non-commuting solution.
  \vfill
  \textbf{1. Construct A:} We need a 3x3 matrix with eigenvalues from $\{i, -i, 0\}$. The simplest is:
  \[ A = \begin{pmatrix} i & 0 & 0 \\ 0 & -i & 0 \\ 0 & 0 & 0 \end{pmatrix} \]
  \vfill
  \textbf{2. Choose Parameters for X:}
  \begin{itemize}
    \item To make it non-commuting, we need the parameter $r > 0$. Let's choose the simplest case: $\mathbf{r=1}$.
    \item This means the block $F$ is a 1x1 invertible matrix. Let's choose $\mathbf{F = [1]}$.
    \item The formula dictates $G = -\frac{3}{4}F^{-1} = [-\frac{3}{4}]$.
    \item For simplicity, we set all other free parameters to zero ($v=0, t=0, C_i=0, D_i=0, W=0$).
  \end{itemize}
\end{frame}

\begin{frame}{A Concrete 3x3 Example: The Result}
  Plugging our simple parameters into the general formula yields a clean, concrete solution for $X$.
  \begin{block}{The Resulting Solution X}
  \[ X = \begin{pmatrix} -i/2 & 1 & 0 \\ -3/4 & i/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} \]
  \end{block}
  This matrix is constructed to be a solution. Now, let's explicitly verify the key properties.
\end{frame}

\begin{frame}{A Concrete 3x3 Example: Verification}
  \textbf{1. Is it non-commuting?} Let's check the products $AX$ and $XA$:
    \[ AX = \begin{pmatrix} 1/2 & i & 0 \\ 3i/4 & 1/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} \quad \neq \quad XA = \begin{pmatrix} 1/2 & -i & 0 \\ -3i/4 & 1/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} \]
    They do not commute, so this is a non-trivial solution.
    \vfill
  \textbf{2. Does it satisfy the YBME?}
    A quick calculation confirms that:
    \[ AXA = \begin{pmatrix} 1/2 & i & 0 \\ 3i/4 & 1/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} A = \begin{pmatrix} i/2 & 1 & 0 \\ -3/4 & -i/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} \]
    \[ XAX = X \begin{pmatrix} 1/2 & -i & 0 \\ -3i/4 & 1/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} = \begin{pmatrix} i/2 & 1 & 0 \\ -3/4 & -i/2 & 0 \\ 0 & 0 & 0 \end{pmatrix} \]
    The equation $AXA=XAX$ holds.
\end{frame}

\section{Generalization and Application}

\begin{frame}{Generalizing the Structure: $A^3 = tA$}
  This successful method can be generalized. Consider the annihilating polynomial:
  \[ p(x) = x^3 - tx = x(x - \sqrt{t})(x + \sqrt{t}) \]
  As long as we know that $t \neq 0$ it is easy to see that:
  \[ \lambda^2 - \lambda\mu + \mu^2 = 3t \neq 0 \]
  Hence we can construct all solutions as above.
\end{frame}

\begin{frame}{Further Generalization}
  The logical next step is to consider higher powers of $A$.
  We can consider the following annihilating polynomial:
  \[ p(x) = x^k - xt = x(x^{k - 1} - t) \]
  This gives us the following two cases.
\end{frame}

\begin{frame}{Generalization: Case 1 ($t \neq 0$)}
  \begin{itemize}
    \item If $t \neq 0$, the roots of $x^{k-1} - t$ are the $(k-1)$-th roots of $t$. These are all distinct from each other and from 0.
    \item The polynomial $p(x)$ has $k$ distinct roots.
  \end{itemize}
  \begin{block}{Structural Theorem}
    For $t \neq 0$, any matrix $A$ satisfying $A^k = tA$ is \textbf{diagonalizable}. Its spectrum is a subset of $\{0, \text{the } (k-1)\text{-th roots of } t\}$.
  \end{block}
\end{frame}

\begin{frame}{Generalization: Case 2 ($t = 0$)}
  \begin{itemize}
    \item If $t = 0$, the equation becomes $A^k = 0$.
    \item The matrix $A$ is \textbf{nilpotent}.
    \item The annihilating polynomial is $p(x) = x^k$, which has a repeated root at 0.
  \end{itemize}
  \begin{alertblock}{Important Distinction}
    A non-zero nilpotent matrix is \textbf{never diagonalizable}. This case requires entirely different tools.
  \end{alertblock}
    \vfill
  \begin{itemize}
    \item The YBME for nilpotent matrices is an active area of research.
    \item Solutions have been fully classified for the simple cases $A^2=0$ and $A^3=0$. \cite{nilpotent2, nilpotent3}
  \end{itemize}
\end{frame}

\begin{frame}{Non-Commuting Solutions}
  \begin{itemize}
    \item The commuting case for diagonalizable matrices is well-understood. \cite{diagonalizable}
    \item The real challenge is finding \textbf{non-commuting solutions}.
    \item The proof of the theorem we used is very specific to 3 eigenvalues and is hard to generalize.
  \end{itemize}
  \vfill
  \textbf{New Strategy:} Instead of generalizing the proof, let's generalize the \textit{problem setup} using our structural theorem.
\end{frame}

\begin{frame}{A Proven Method}
  The literature provides a clear path forward: classify solutions based on the minimal polynomial of $A$.
  \begin{itemize}
    \item A successful paper on quadrapotent matrices ($A^4=A$) used a powerful block-matrix method \cite{quadrapotent}.
    \item \textbf{The Method:} Treat $X$ as a $2 \times 2$ block matrix and solve the resulting system of four coupled matrix equations.
  \end{itemize}
  \begin{block}{Strength of this Approach}
    This method is exhaustive. When it can be carried out, it yields a \textit{complete} classification of all solutions for a given minimal polynomial.
  \end{block}
\end{frame}

\begin{frame}{The Limitation}
  While powerful, the block-matrix method has a critical bottleneck.
  \begin{alertblock}{The Challenge: Exploding Complexity}
  The complexity of the coupled equation system explodes as the number of distinct eigenvalues in the minimal polynomial increases.
  \end{alertblock}
  \begin{itemize}
    \item This is precisely why the most complex cases in the $A^4=A$ paper—those with three or four distinct non-zero eigenvalues—were left as open problems.
    \item This reveals the frontier of the current research: our methods are strong for simple spectra, but break down as the algebraic complexity grows.
  \end{itemize}
\end{frame}

\begin{frame}{From Powers to Polynomials}
  My initial work on $A^k=tA$ led to a crucial insight: the power $k$ is a convenient starting point, but the true object of study is the \textbf{minimal polynomial} of $A$.
  \begin{itemize}
    \item The condition $A^k=tA$ is simply a way to generate matrices whose minimal polynomials have distinct roots and divide $x^k-tx$.
    \pause
    \item This reframes the research: our goal is to classify YBME solutions for matrices with increasingly complex minimal polynomials.
  \end{itemize}
\end{frame}

\begin{frame}{The Problem with Classifying by Powers: Redundancy}
  Focusing on the power $k$ can be misleading because it introduces redundancies.
  \begin{itemize}
    \item For example, if a matrix $A$ is idempotent ($A^2=A$), it is also tripotent ($A^3=A$), quadrapotent ($A^4=A$), and so on.
    \[ A^3 = A^2 \cdot A = A \cdot A = A^2 = A \]
    \item This means the set of solutions for $A^k=A$ contains many solutions from simpler, lower-power cases.
    \pause
    \item This "contamination" makes it difficult to identify what is genuinely new about a given power $k$.
  \end{itemize}
\end{frame}

\begin{frame}{Classifying by Polynomials}
  Switching our focus to the minimal polynomial removes these redundancies and clarifies the problem's structure.
  \begin{itemize}
    \item The minimal polynomial is the unique, most efficient descriptor of the matrix's algebraic properties.
    \item It allows us to study the "pure" structures without the baggage of simpler cases.
  \end{itemize}
  \begin{alertblock}{The True Measure of Complexity}
  Each new distinct, non-zero eigenvalue added to the minimal polynomial represents a fundamentally new level of difficulty. The research program is not about increasing $k$, but about increasing the degree of the minimal polynomial.
  \end{alertblock}
\end{frame}

\begin{frame}{Classifying by Polynomials}
  This insight leads to two approachs for future work.
  \begin{columns}[T]
    \begin{column}{.5\textwidth}
      \begin{block}{First Approach: Systematic Classification}
        Methodically apply the proven block-matrix technique to classify solutions for minimal polynomials of increasing complexity:
        \begin{itemize}
            \item 2 distinct eigenvalues (e.g., $x(x-\lambda)$)
            \item 3 distinct eigenvalues (e.g., $x(x-\lambda)(x-\mu)$)
            \item 4 distinct eigenvalues...
        \end{itemize}
        The challenge is the known explosion in algebraic complexity.
      \end{block}
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{alertblock}{Second Approach: A Constructive Approach}
        This is a more ambitious, structural question:
        \vfill
        Can we use the solutions for simple polynomials as building blocks to construct solutions for more complex ones?
        \vfill
        For example, can solutions for $m_1(x)=x(x-\lambda_1)$ and $m_2(x)=x(x-\lambda_2)$ be "stitched together" to form solutions for $m_3(x)=x(x-\lambda_1)(x-\lambda_2)$?
      \end{alertblock}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Breaking Diagonalizability}
  Finally, this entire discussion has been within the "safe" world of diagonalizable matrices, by insisting on distinct roots in the minimal polynomial.
  \vfill
  The most challenging and exciting open questions lie beyond this boundary.
  \begin{block}{A Fundamental Open Question}
  What happens when we introduce multiplicity?
  \vfill
  For example, what can we say about YBME solutions for a matrix $A$ with a minimal polynomial like:
  \[ m_A(x) = x^2(x-1) \]
  Such a matrix is \textbf{not diagonalizable}. This would most likely require completely different techniques.
  \end{block}
\end{frame}


% --- FINAL SLIDE ---
\begin{frame}
  \begin{center}
    \Huge Thank You!
    \vfill
    Questions?
  \end{center}
\end{frame}

\begin{frame}[allowframebreaks]{References}
  \bibliography{k-potent-matrices-refs}
\end{frame}

\end{document}

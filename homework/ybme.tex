\documentclass{article}

\input{homework-preamble.tex}

\title{Yang-Baxter-like matrix equation}
\author{Mihailo Đurić}
\date{\today}

\begin{document}

\maketitle
\newpage

\begin{problem}
  Let
  \[A = \begin{bmatrix} 0 & 1 & 0\\ 1 & 1 & 0\\ 1 & 0 & 0 \end{bmatrix}.\]
  \begin{itemize}
    \item Find $P$ and $Q$ such that $PA = AQ = 0$.
    \item Find some inner inverse of $A$.
    \item Find one solution to YBME for this particular $A$.
      Try to generate some new classes of solutions with it.
  \end{itemize}
\end{problem}

\begin{solution}
  We are looking for a matrix $P = \begin{bmatrix} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}\\ a_{31} & a_{32} & a_{33} \end{bmatrix}$ such that
  \[PA = \begin{bmatrix} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}\\ a_{31} & a_{32} & a_{33} \end{bmatrix} \cdot \begin{bmatrix} 0 & 1 & 0\\ 1 & 1 & 0\\ 1 & 0 & 0 \end{bmatrix} = \begin{bmatrix} a_{12} + a_{13} & a_{11} + a_{12} & 0\\ a_{22} + a_{23} & a_{21} + a_{22} & 0\\ a_{32} + a_{33} & a_{31} + a_{32} & 0 \end{bmatrix} = 0.\]
  Thus we have $a_{i1} = - a_{i2} = a_{i3}$ for $i \in \{1, 2, 3\}$.
  Choosing $a_{i1} = 1$ we get
  \[P = \begin{bmatrix} 1 & -1 & 1\\ 1 & -1 & 1\\ 1 & -1 & 1 \end{bmatrix}.\]
  Similarly we get that $Q$ is of the form
  \[\begin{bmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ a_{31} & a_{32} & a_{33} \end{bmatrix},\]
  for any $a_{3i} \in \C$.
  Thus we can simply choose
  \[Q = \begin{bmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ 1 & 1 & 1 \end{bmatrix}.\]
  Thus we have have $PA = AQ = 0$.

  We put
  \[B = \begin{bmatrix} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}\\ a_{31} & a_{32} & a_{33} \end{bmatrix}.\]
  Then we have
  \[ABA = \begin{bmatrix} a_{22} + a_{23} & a_{21} + a_{22} & 0\\ a_{12} + a_{13} + a_{22} + a_{23} & a_{11} + a_{12} + a_{21} + a_{22} & 0\\ a_{12} + a_{13} & a_{11} + a_{12} & 0 \end{bmatrix} = A.\]
  Thus we have $a_{22} = - a_{23} = 1 - a_{21}$, $a_{12} = 1 - a_{13} = - a_{11}$.
  Picking $a_{22} = a_{12} = 1$ and $a_{3j} = 0$ we have
  \[B = \begin{bmatrix} -1 & 1 & 0\\ 0 & 1 & -1\\ 0 & 0 & 0 \end{bmatrix},\]
  and $B$ is an inner inverse of $A$.

  We already know that $P$ and $Q$ are solutions to YBME, thus from the Lemma on page 6 we know that any powers of $P$ and $Q$ are also solutions.
  Further more, we know that any matrix $Q^n P^m$ is a solution for any $n, m \in \N$.
  We can also insert $B$ anywhere in that solution by the theorem on page 7, so we have $Q^{n - i} B Q^i P^m$ and $Q^n P^j B P^{m - j}$ as families of solutions.
  We can continue inserting $B$, either raising its power or inserting it in a different place now to generate new solutions.
  We can also easily generate new matrices $P$, $Q$ and $B$ and repeat the process, or mix the solutions.
\end{solution}

\begin{problem}
  A square matrix $L$ is nilpotent if and only if $\sigma(L) = \{0\}$.
\end{problem}

\begin{solution}
  First we assume that $L$ is nilpotent, i.e., that there exists some $n \in \N$ such that $L^n = 0$.
  We consider the polynomial $p(A) = A^n$.
  We clearly see that $p(L) = 0$, thus $\sigma(p(L)) = 0$.
  By the spectral mapping theorem we have $p(\sigma(L)) = 0$, so for any eigenvalue $\lambda$ of $L$ we know that $\lambda^n = 0$.
  From here we clearly have $\lambda = 0$.
  
  Conversely, assume that $\sigma(L) = \{0\}$.
  We have that the characteristic polynomial of $L$ is $p(\lambda) = \lambda^k$, where $k$ is the dimension of $L$.
  By the Cayley-Hamilton theorem, we know that $p(L) = 0$, thus we have $L^k = 0$.
  Hence, $L$ is nilpotent.
\end{solution}

\begin{problem}
  Let $A \neq 0$ be a nilpotent matrix with its nilpotency index equal to $k > 1$, and let $X_0$ be a solution to YBME.
  Then, for any $\alpha \in \C$ the matrix $X_0 + \alpha A^{k - 1}$ is also a solution.
\end{problem}

\begin{solution}
  Let $A^k = 0$ and $A X_0 A = X_0 A X_0$.
  Now we consider
  \[A (X_0 + \alpha A^{k - 1}) A = A X_0 A + \alpha A A^{k - 1} A = X_0 A X_0 \alpha A \cdots 0 = X_0 A X_0.\]
  Similarly we have
  \[(X_0 + \alpha A^{k - 1}) A (X_0 + \alpha A^{k - 1}) = (X_0 A + \alpha A^k) (X_0 + \alpha A^{k - 1}) = X_0 A (X_0 + \alpha A^{k - 1}) = X_0 A X_0 + \alpha X A^k = X_0 A X_0.\]
  Thus we see that $(X_0 + \alpha A^{k - 1})$ is also a solution.
\end{solution}

\begin{problem}
  If $A$ is a regular matrix, what can be said about the core-nilpotent decomposition of $A$?
\end{problem}

\begin{solution}
  Since $A$ is regular it is clear that its null-space is trivial and its range is the whole space.
  We also know that every power of $A$ is invertible, if $A^{-1}$ is an inverse of $A$ then it is easy to see that $(A^{-1})^k$ is the inverse of $A^k$, for any $k$.
  Thus every power of $A$ also has a trivial null-space and range.
  Thus $p = 0$ is the smallest integer such that $N(A^{p + 1}) = N(A^p)$ and $R(A^{p + 1}) = R(A^p)$.
  Since $A^0 = I$ for any matrix, it is clear that no matrix with a nilpotency index $0$ can exist.
  Thus we require $A = A_2$.
  So a regular matrix decomposes into itself.
\end{solution}

\begin{problem}
  If $A$ is a nilpotent matrix, with the nilpotency index equal to $m$, what can be said about the core-nilpotent decomposition of $A$?
\end{problem}

\begin{solution}
  By the definition of the nilpotency index, it is clear that the index of $A$ is $m$.
  Thus we have $A_2 \in R(A^m)$ is invertible, or $A_2 = 0$ since $R(A^m)$ is trivial.
  However since $0$ is not invertible, we cannot have any block $A_2$, hence $A = A_1$.
  We see that if $A$ is nilpotent, then its core-nilpotent decomposition is simply $A$.
\end{solution}

\begin{problem}
  Let $A = A_1 \oplus A_2$ be the core-nilpotent decomposition of $A$ with respect to the notation and assumptions from (2).
  Prove that $A_1^n = 0$ if and only if $n \ge p$, where $p$ is the smallest integer such that $N(A^p) \oplus R(A^p) = \C^n$.
  On the other hand, show that the spectrum of $A_2$ consists precisely out of the nonzero eigenvalues of $A$.
  What about the corresponding eigenspaces?
\end{problem}

\begin{solution}
  If we have $A_1^n = 0$, then clearly $n \ge p$, otherwise $p > n$ contradicts the minimality of the index $p$.

  Conversely, if $A_1^p = 0$ and $n \ge p$, then $A_1^n = A_1^{p} A_1^{n - p} = 0 A_1^{n - p} = 0$.

  First we convert $A$ to its Jordan Normal Form, so we have
  \[J_A = S A S^{- 1} = S \begin{bmatrix} A_1 & 0\\ 0 & A_2 \end{bmatrix} S^{- 1}.\]
  Next we transform $A_1$ and $A_2$ to their Jordan Normal Forms, let $J_{A_1} = Q A_1 Q^{- 1}$ and $J_{A_2} = T A_2 T^{- 1}$.
  We can then easily verify that
  \[J_A = S \begin{bmatrix} A_1 & 0\\ 0 & A_2 \end{bmatrix} S^{- 1} = S \begin{bmatrix} Q & 0\\ 0 & T \end{bmatrix} \begin{bmatrix} J_{A_1} & 0\\ 0 & J_{A_2} \end{bmatrix} \begin{bmatrix} Q^{- 1} & 0\\ 0 & T^{- 1} \end{bmatrix} S^{- 1}.\]
  It is also clear to see that
  \[\begin{bmatrix} Q & 0\\ 0 & T \end{bmatrix} \begin{bmatrix} Q^{- 1} & 0\\ 0 & T^{- 1} \end{bmatrix} = I.\]
  We know that the diagonal of $J_A$ is comprised of the eigenvalues of $A$, and that $diag(J_A) = diag(J_{A_1}) \cup diag(J_{A_2})$.
  Since $A_1$ is a nilpotent matrix, we know that all of its eigenvalues are 0.
  Thus $J_{A_2}$ must contain all the nonzero eigenvalues, and it cannot have any zero eigenvalues as it is invertible.
  Since we know that $\sigma{A_2} = \sigma{J_{A_2}}$, we are done.

  Let $A_1$ be a $k \times k$ matrix and $A_2$ a $(n - k) \times (n - k)$ matrix.
  Assume that $v$ is any eigenvector of $A$.
  Then we have
  \[\begin{bmatrix} A_1 & 0\\ 0 & A_2 \end{bmatrix} v = \lambda v,\]
  for some eigenvalue $\lambda$.
  First, assume that $\lambda \neq 0$.
  We can write $v$ in a block form as $v = \begin{bmatrix} v_1\\ v_2 \end{bmatrix}$.
  Then we have
  \[\begin{bmatrix} A_1 & 0\\ 0 & A_2 \end{bmatrix} \begin{bmatrix} v_1\\ v_2 \end{bmatrix} = \begin{bmatrix} A_1 v_1\\ A_2 v_2 \end{bmatrix} =  \begin{bmatrix} \lambda v_1\\ \lambda v_2 \end{bmatrix}.\]
  Thus we have a system of equations
  \[\begin{cases} A_1 v_1 = \lambda v_1,\\ A_2 v_2 = \lambda v_2 \end{cases}.\]
  Since $A_1$ is nilpotent its only eigenvalue is $0$, and since $\lambda \neq 0$, we have $v_1 = 0$.
  We can see that $v_2$ is the eigenvector corresponding to the eigenvalue $\lambda$ of $A_2$.
  In this case, we see that eigenspace for $\lambda$ of $A_2$ is isomorphic to the eigenspace of $A$, by the function $f(v) = \begin{bmatrix} 0\\ v \end{bmatrix}$, where $0$ represents an $k \times 1$ block matrix consisting of zeros.
  
  Now, if our $\lambda = 0$, then $v_2 = 0$ and $A_1 v_1 = 0$.
  Thus the eigenspace for $0$ of $A$ is isomorphic with the null-space of $A_1$.
\end{solution}

\end{document}
